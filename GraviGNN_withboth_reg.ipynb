{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraviGNN â€“ Physics-Informed Gravity Inversion\n",
    "\n",
    "* Loads gravity anomaly data and corresponding 3D density models\n",
    "* Preprocesses and normalizes the dataset\n",
    "* Defines a graph-enhanced GraviGNN architecture\n",
    "* Performs 2D gravity â†’ 3D density reconstruction\n",
    "* Ensures physical consistency through forward modeling\n",
    "* Evaluates performance using error and RÂ² metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as colors\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "plt.rc('font',family='Times New Roman', size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters & Physical Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep network parameters\n",
    "patience = 20\n",
    "epochs = 130\n",
    "tra_num = 20000\n",
    "val_num = 2000\n",
    "part_num = 100\n",
    "category = 6\n",
    "batch_size = 16\n",
    "num_cell = 32\n",
    "learning_rate = 4e-4\n",
    "threshold = 1e-4\n",
    "realdata_num = 1\n",
    "start_fm = 32\n",
    "syn_num = part_num*category\n",
    "total_num = tra_num + val_num\n",
    "\n",
    "\n",
    "# Physical parameters\n",
    "density = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data folders\n",
    "dataFile = './data/tra&val/data{}.mat'\n",
    "syn_dataFile = './data/syn/data{}.mat'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(total_num):\n",
    "    data = h5py.File(dataFile.format(i), 'r')\n",
    "    m = data['m'][0] / density\n",
    "    d = data['d'][0]\n",
    "    d = np.nan_to_num(d)\n",
    "    x.append(d.reshape(1, num_cell, num_cell))\n",
    "    y.append(m.reshape(16, num_cell, num_cell))\n",
    "\n",
    "syn_x = []\n",
    "syn_y = []\n",
    "for i in range(syn_num):\n",
    "    data = h5py.File(syn_dataFile.format(i), 'r')\n",
    "    m = data['m'][0] / density\n",
    "    d = data['d'][0]\n",
    "    d = np.nan_to_num(d)\n",
    "    syn_x.append(d.reshape(1, num_cell, num_cell))\n",
    "    syn_y.append(m.reshape(16, num_cell, num_cell))\n",
    "\n",
    "tra_x = x[:tra_num]\n",
    "tra_y = y[:tra_num]\n",
    "val_x = x[-val_num:]\n",
    "val_y = y[-val_num:]\n",
    "\n",
    "tra_idxs = list(range(len(tra_x)))\n",
    "val_idxs = list(range(len(val_x)))\n",
    "# np.random.shuffle(tra_idxs)\n",
    "# np.random.shuffle(val_idxs)\n",
    "syn_idxs = list(range(len(syn_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, train=True, masks=None):\n",
    "        self.train = train\n",
    "        self.images = images\n",
    "        if self.train:\n",
    "            self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = None\n",
    "        if self.train:\n",
    "            mask = self.masks[idx]\n",
    "        return (image, mask)\n",
    "\n",
    "\n",
    "tra = Dataset(np.array(tra_x).astype(np.float32)[tra_idxs], train=True,\n",
    "              masks=np.array(tra_y).astype(np.float32)[tra_idxs])\n",
    "val = Dataset(np.array(val_x).astype(np.float32)[val_idxs], train=True,\n",
    "              masks=np.array(val_y).astype(np.float32)[val_idxs])\n",
    "syn = Dataset(np.array(syn_x).astype(np.float32)[syn_idxs], train=True,\n",
    "              masks=np.array(syn_y).astype(np.float32)[syn_idxs])\n",
    "\n",
    "tra_loader = torch.utils.data.DataLoader(dataset=tra, batch_size=batch_size, shuffle=False, pin_memory=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=batch_size, shuffle=False, pin_memory=False)\n",
    "syn_loader = torch.utils.data.DataLoader(dataset=syn, batch_size=batch_size, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraviGNN with Forward-fitting and Smoothness Regularizers.\n",
    "* Implements **GraviGNN**\n",
    "* Uses `double_conv` blocks for spatial feature extraction\n",
    "* Applies **GraphConv with k-NN aggregation** for global feature learning\n",
    "* Outputs a **16-layer 3D density model** from 2D gravity input\n",
    "* Defines a **Physics-Informed Loss (PINN)** combining:\n",
    "\n",
    "  * Dice loss (supervised reconstruction)\n",
    "  * Physics/data fidelity loss using forward matrix **G**\n",
    "* Enforces gravity consistency: ( d_{pred} = G m_{pred} )\n",
    "* Trains using Adam optimizer with validation and synthetic testing\n",
    "* Saves trained model weights after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Tra: 0.8416 | Val: 0.6685 | Syn: 0.6291\n",
      "Epoch: 002 | Tra: 0.7146 | Val: 0.6211 | Syn: 0.6154\n",
      "Epoch: 003 | Tra: 0.5638 | Val: 0.5133 | Syn: 0.4995\n",
      "Epoch: 004 | Tra: 0.4595 | Val: 0.4222 | Syn: 0.4023\n",
      "Epoch: 005 | Tra: 0.3976 | Val: 0.3683 | Syn: 0.3407\n",
      "Epoch: 006 | Tra: 0.3622 | Val: 0.3438 | Syn: 0.3091\n",
      "Epoch: 007 | Tra: 0.3406 | Val: 0.3250 | Syn: 0.2857\n",
      "Epoch: 008 | Tra: 0.3258 | Val: 0.3123 | Syn: 0.2607\n",
      "Epoch: 009 | Tra: 0.3157 | Val: 0.3012 | Syn: 0.2492\n",
      "Epoch: 010 | Tra: 0.3068 | Val: 0.2982 | Syn: 0.2443\n",
      "Epoch: 011 | Tra: 0.2996 | Val: 0.2874 | Syn: 0.2331\n",
      "Epoch: 012 | Tra: 0.2938 | Val: 0.2905 | Syn: 0.2287\n",
      "Epoch: 013 | Tra: 0.2886 | Val: 0.2793 | Syn: 0.2220\n",
      "Epoch: 014 | Tra: 0.2842 | Val: 0.2757 | Syn: 0.2176\n",
      "Epoch: 015 | Tra: 0.2804 | Val: 0.2745 | Syn: 0.2140\n",
      "Epoch: 016 | Tra: 0.2773 | Val: 0.2712 | Syn: 0.2081\n",
      "Epoch: 017 | Tra: 0.2732 | Val: 0.2715 | Syn: 0.2077\n",
      "Epoch: 018 | Tra: 0.2693 | Val: 0.2673 | Syn: 0.2138\n",
      "Epoch: 019 | Tra: 0.2620 | Val: 0.2616 | Syn: 0.2042\n",
      "Epoch: 020 | Tra: 0.2570 | Val: 0.2563 | Syn: 0.2018\n",
      "Epoch: 021 | Tra: 0.2538 | Val: 0.2516 | Syn: 0.1959\n",
      "Epoch: 022 | Tra: 0.2508 | Val: 0.2517 | Syn: 0.2002\n",
      "Epoch: 023 | Tra: 0.2484 | Val: 0.2482 | Syn: 0.1909\n",
      "Epoch: 024 | Tra: 0.2458 | Val: 0.2484 | Syn: 0.1969\n",
      "Epoch: 025 | Tra: 0.2440 | Val: 0.2464 | Syn: 0.1873\n",
      "Epoch: 026 | Tra: 0.2421 | Val: 0.2447 | Syn: 0.1875\n",
      "Epoch: 027 | Tra: 0.2404 | Val: 0.2440 | Syn: 0.1845\n",
      "Epoch: 028 | Tra: 0.2385 | Val: 0.2440 | Syn: 0.1840\n",
      "Epoch: 029 | Tra: 0.2371 | Val: 0.2408 | Syn: 0.1777\n",
      "Epoch: 030 | Tra: 0.2360 | Val: 0.2419 | Syn: 0.1832\n",
      "Epoch: 031 | Tra: 0.2345 | Val: 0.2403 | Syn: 0.1808\n",
      "Epoch: 032 | Tra: 0.2333 | Val: 0.2395 | Syn: 0.1801\n",
      "Epoch: 033 | Tra: 0.2319 | Val: 0.2398 | Syn: 0.1808\n",
      "Epoch: 034 | Tra: 0.2304 | Val: 0.2390 | Syn: 0.1781\n",
      "Epoch: 035 | Tra: 0.2292 | Val: 0.2380 | Syn: 0.1770\n",
      "Epoch: 036 | Tra: 0.2281 | Val: 0.2370 | Syn: 0.1805\n",
      "Epoch: 037 | Tra: 0.2270 | Val: 0.2378 | Syn: 0.1771\n",
      "Epoch: 038 | Tra: 0.2260 | Val: 0.2368 | Syn: 0.1803\n",
      "Epoch: 039 | Tra: 0.2250 | Val: 0.2357 | Syn: 0.1755\n",
      "Epoch: 040 | Tra: 0.2241 | Val: 0.2354 | Syn: 0.1784\n",
      "Epoch: 041 | Tra: 0.2226 | Val: 0.2346 | Syn: 0.1747\n",
      "Epoch: 042 | Tra: 0.2221 | Val: 0.2364 | Syn: 0.1786\n",
      "Epoch: 043 | Tra: 0.2209 | Val: 0.2345 | Syn: 0.1727\n",
      "Epoch: 044 | Tra: 0.2205 | Val: 0.2350 | Syn: 0.1711\n",
      "Epoch: 045 | Tra: 0.2191 | Val: 0.2336 | Syn: 0.1717\n",
      "Epoch: 046 | Tra: 0.2181 | Val: 0.2345 | Syn: 0.1694\n",
      "Epoch: 047 | Tra: 0.2173 | Val: 0.2351 | Syn: 0.1702\n",
      "Epoch: 048 | Tra: 0.2165 | Val: 0.2369 | Syn: 0.1696\n",
      "Epoch: 049 | Tra: 0.2158 | Val: 0.2379 | Syn: 0.1764\n",
      "Epoch: 050 | Tra: 0.2153 | Val: 0.2359 | Syn: 0.1726\n",
      "Epoch: 051 | Tra: 0.2143 | Val: 0.2329 | Syn: 0.1682\n",
      "Epoch: 052 | Tra: 0.2138 | Val: 0.2364 | Syn: 0.1720\n",
      "Epoch: 053 | Tra: 0.2126 | Val: 0.2378 | Syn: 0.1745\n",
      "Epoch: 054 | Tra: 0.2118 | Val: 0.2384 | Syn: 0.1768\n",
      "Epoch: 055 | Tra: 0.2101 | Val: 0.2381 | Syn: 0.1780\n",
      "Epoch: 056 | Tra: 0.2084 | Val: 0.2334 | Syn: 0.1688\n",
      "Epoch: 057 | Tra: 0.2075 | Val: 0.2376 | Syn: 0.1784\n",
      "Epoch: 058 | Tra: 0.2070 | Val: 0.2396 | Syn: 0.1782\n",
      "Epoch: 059 | Tra: 0.2064 | Val: 0.2357 | Syn: 0.1742\n",
      "Epoch: 060 | Tra: 0.2053 | Val: 0.2380 | Syn: 0.1733\n",
      "Epoch: 061 | Tra: 0.2044 | Val: 0.2362 | Syn: 0.1702\n",
      "Epoch: 062 | Tra: 0.2042 | Val: 0.2364 | Syn: 0.1723\n",
      "Epoch: 063 | Tra: 0.2033 | Val: 0.2385 | Syn: 0.1763\n",
      "Epoch: 064 | Tra: 0.2019 | Val: 0.2356 | Syn: 0.1717\n",
      "Epoch: 065 | Tra: 0.2018 | Val: 0.2341 | Syn: 0.1697\n",
      "Epoch: 066 | Tra: 0.2012 | Val: 0.2343 | Syn: 0.1703\n",
      "Epoch: 067 | Tra: 0.2001 | Val: 0.2355 | Syn: 0.1719\n",
      "Epoch: 068 | Tra: 0.1994 | Val: 0.2398 | Syn: 0.1800\n",
      "Epoch: 069 | Tra: 0.1982 | Val: 0.2360 | Syn: 0.1711\n",
      "Epoch: 070 | Tra: 0.1974 | Val: 0.2386 | Syn: 0.1778\n",
      "Epoch: 071 | Tra: 0.1964 | Val: 0.2368 | Syn: 0.1725\n",
      "Epoch: 072 | Tra: 0.1954 | Val: 0.2357 | Syn: 0.1729\n",
      "Epoch: 073 | Tra: 0.1951 | Val: 0.2366 | Syn: 0.1689\n",
      "Epoch: 074 | Tra: 0.1936 | Val: 0.2369 | Syn: 0.1706\n",
      "Epoch: 075 | Tra: 0.1931 | Val: 0.2345 | Syn: 0.1703\n",
      "Epoch: 076 | Tra: 0.1921 | Val: 0.2340 | Syn: 0.1706\n",
      "Epoch: 077 | Tra: 0.1913 | Val: 0.2388 | Syn: 0.1727\n",
      "Epoch: 078 | Tra: 0.1914 | Val: 0.2346 | Syn: 0.1690\n",
      "Epoch: 079 | Tra: 0.1906 | Val: 0.2349 | Syn: 0.1668\n",
      "Epoch: 080 | Tra: 0.1902 | Val: 0.2339 | Syn: 0.1668\n",
      "Epoch: 081 | Tra: 0.1894 | Val: 0.2321 | Syn: 0.1657\n",
      "Epoch: 082 | Tra: 0.1897 | Val: 0.2330 | Syn: 0.1653\n",
      "Epoch: 083 | Tra: 0.1885 | Val: 0.2359 | Syn: 0.1681\n",
      "Epoch: 084 | Tra: 0.1880 | Val: 0.2370 | Syn: 0.1713\n",
      "Epoch: 085 | Tra: 0.1873 | Val: 0.2385 | Syn: 0.1786\n",
      "Epoch: 086 | Tra: 0.1863 | Val: 0.2393 | Syn: 0.1704\n",
      "Epoch: 087 | Tra: 0.1856 | Val: 0.2392 | Syn: 0.1731\n",
      "Epoch: 088 | Tra: 0.1847 | Val: 0.2396 | Syn: 0.1763\n",
      "Epoch: 089 | Tra: 0.1835 | Val: 0.2409 | Syn: 0.1716\n",
      "Epoch: 090 | Tra: 0.1831 | Val: 0.2394 | Syn: 0.1723\n",
      "Epoch: 091 | Tra: 0.1824 | Val: 0.2402 | Syn: 0.1747\n",
      "Epoch: 092 | Tra: 0.1815 | Val: 0.2371 | Syn: 0.1697\n",
      "Epoch: 093 | Tra: 0.1818 | Val: 0.2424 | Syn: 0.1729\n",
      "Epoch: 094 | Tra: 0.1813 | Val: 0.2388 | Syn: 0.1691\n",
      "Epoch: 095 | Tra: 0.1805 | Val: 0.2394 | Syn: 0.1675\n",
      "Epoch: 096 | Tra: 0.1794 | Val: 0.2380 | Syn: 0.1734\n",
      "Epoch: 097 | Tra: 0.1788 | Val: 0.2405 | Syn: 0.1695\n",
      "Epoch: 098 | Tra: 0.1782 | Val: 0.2409 | Syn: 0.1775\n",
      "Epoch: 099 | Tra: 0.1779 | Val: 0.2416 | Syn: 0.1735\n",
      "Epoch: 100 | Tra: 0.1772 | Val: 0.2391 | Syn: 0.1645\n",
      "Epoch: 101 | Tra: 0.1769 | Val: 0.2391 | Syn: 0.1673\n",
      "Epoch: 102 | Tra: 0.1763 | Val: 0.2418 | Syn: 0.1715\n",
      "Epoch: 103 | Tra: 0.1761 | Val: 0.2428 | Syn: 0.1744\n",
      "Epoch: 104 | Tra: 0.1761 | Val: 0.2427 | Syn: 0.1743\n",
      "Epoch: 105 | Tra: 0.1755 | Val: 0.2416 | Syn: 0.1726\n",
      "Epoch: 106 | Tra: 0.1745 | Val: 0.2445 | Syn: 0.1716\n",
      "Epoch: 107 | Tra: 0.1734 | Val: 0.2446 | Syn: 0.1744\n",
      "Epoch: 108 | Tra: 0.1733 | Val: 0.2440 | Syn: 0.1758\n",
      "Epoch: 109 | Tra: 0.1728 | Val: 0.2436 | Syn: 0.1723\n",
      "Epoch: 110 | Tra: 0.1724 | Val: 0.2431 | Syn: 0.1699\n",
      "Epoch: 111 | Tra: 0.1718 | Val: 0.2441 | Syn: 0.1770\n",
      "Epoch: 112 | Tra: 0.1713 | Val: 0.2448 | Syn: 0.1786\n",
      "Epoch: 113 | Tra: 0.1710 | Val: 0.2451 | Syn: 0.1737\n",
      "Epoch: 114 | Tra: 0.1705 | Val: 0.2461 | Syn: 0.1795\n",
      "Epoch: 115 | Tra: 0.1700 | Val: 0.2442 | Syn: 0.1686\n",
      "Epoch: 116 | Tra: 0.1698 | Val: 0.2435 | Syn: 0.1733\n",
      "Epoch: 117 | Tra: 0.1695 | Val: 0.2455 | Syn: 0.1754\n",
      "Epoch: 118 | Tra: 0.1690 | Val: 0.2454 | Syn: 0.1735\n",
      "Epoch: 119 | Tra: 0.1685 | Val: 0.2440 | Syn: 0.1718\n",
      "Epoch: 120 | Tra: 0.1679 | Val: 0.2461 | Syn: 0.1708\n",
      "Epoch: 121 | Tra: 0.1674 | Val: 0.2490 | Syn: 0.1780\n",
      "Epoch: 122 | Tra: 0.1666 | Val: 0.2468 | Syn: 0.1717\n",
      "Epoch: 123 | Tra: 0.1659 | Val: 0.2453 | Syn: 0.1697\n",
      "Epoch: 124 | Tra: 0.1654 | Val: 0.2476 | Syn: 0.1730\n",
      "Epoch: 125 | Tra: 0.1648 | Val: 0.2475 | Syn: 0.1722\n",
      "Epoch: 126 | Tra: 0.1642 | Val: 0.2442 | Syn: 0.1715\n",
      "Epoch: 127 | Tra: 0.1639 | Val: 0.2461 | Syn: 0.1710\n",
      "Epoch: 128 | Tra: 0.1633 | Val: 0.2465 | Syn: 0.1704\n",
      "Epoch: 129 | Tra: 0.1637 | Val: 0.2477 | Syn: 0.1721\n",
      "Epoch: 130 | Tra: 0.1628 | Val: 0.2476 | Syn: 0.1709\n",
      "Epoch: 131 | Tra: 0.1623 | Val: 0.2468 | Syn: 0.1700\n",
      "Training completed in 8589.90 seconds.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "##############################################\n",
    "# 1. Basic Modules & ViG Structure\n",
    "##############################################\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.ELU = nn.ELU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ELU(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads=4, k=8):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k\n",
    "        assert out_dim % num_heads == 0\n",
    "        self.head_dim = out_dim // num_heads\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(2 * in_dim, self.head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        dist = torch.cdist(x, x, p=2)\n",
    "        diag = torch.eye(N, device=x.device).bool().unsqueeze(0)\n",
    "        dist.masked_fill_(diag, float('inf'))\n",
    "\n",
    "        effective_k = self.k if self.k < N else N - 1\n",
    "        knn_indices = torch.topk(-dist, k=effective_k, dim=-1).indices\n",
    "\n",
    "        batch_indices = torch.arange(B, device=x.device).view(B, 1, 1).expand(B, N, effective_k)\n",
    "        neighbors = x[batch_indices, knn_indices]\n",
    "\n",
    "        agg, _ = torch.max(neighbors, dim=2)\n",
    "        concat_feat = torch.cat([x, agg], dim=-1)\n",
    "\n",
    "        return torch.cat([linear(concat_feat) for linear in self.linears], dim=-1)\n",
    "\n",
    "\n",
    "class ViGBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads=4, k=8, ff_hidden_dim=None, dropout=0.1):\n",
    "        super(ViGBlock, self).__init__()\n",
    "        ff_hidden_dim = ff_hidden_dim or in_dim * 2\n",
    "\n",
    "        self.proj_in = nn.Linear(in_dim, in_dim)\n",
    "        self.graph_conv = GraphConv(in_dim, out_dim, num_heads, k)\n",
    "        self.proj_out = nn.Linear(out_dim, out_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(out_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(out_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = self.proj_in(x)\n",
    "        gc = self.dropout(self.activation(self.proj_out(self.graph_conv(x_proj))))\n",
    "        y = self.norm1(x + gc)\n",
    "        return self.norm2(y + self.dropout(self.ffn(y)))\n",
    "\n",
    "\n",
    "class ViGUNet(nn.Module):\n",
    "    def __init__(self, start_fm=32, num_heads=4, dropout=0.1, k=8):\n",
    "        super(ViGUNet, self).__init__()\n",
    "\n",
    "        self.enc1_conv = double_conv(1, start_fm)\n",
    "        self.enc1_vig = ViGBlock(start_fm, start_fm, num_heads, k, dropout=dropout)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2_conv = double_conv(start_fm, start_fm * 2)\n",
    "        self.enc2_vig = ViGBlock(start_fm * 2, start_fm * 2, num_heads, k, dropout=dropout)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc3_conv = double_conv(start_fm * 2, start_fm * 4)\n",
    "        self.enc3_vig = ViGBlock(start_fm * 4, start_fm * 4, num_heads, k, dropout=dropout)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc4_conv = double_conv(start_fm * 4, start_fm * 8)\n",
    "        self.enc4_vig = ViGBlock(start_fm * 8, start_fm * 8, num_heads, k, dropout=dropout)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck_conv = double_conv(start_fm * 8, start_fm * 16)\n",
    "        self.bottleneck_vig = ViGBlock(start_fm * 16, start_fm * 16, num_heads, k, dropout=dropout)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(start_fm * 16, start_fm * 8, 2, 2)\n",
    "        self.dec4_conv = double_conv(start_fm * 16, start_fm * 8)\n",
    "        self.dec4_vig = ViGBlock(start_fm * 8, start_fm * 8, num_heads, k, dropout=dropout)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(start_fm * 8, start_fm * 4, 2, 2)\n",
    "        self.dec3_conv = double_conv(start_fm * 8, start_fm * 4)\n",
    "        self.dec3_vig = ViGBlock(start_fm * 4, start_fm * 4, num_heads, k, dropout=dropout)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(start_fm * 4, start_fm * 2, 2, 2)\n",
    "        self.dec2_conv = double_conv(start_fm * 4, start_fm * 2)\n",
    "        self.dec2_vig = ViGBlock(start_fm * 2, start_fm * 2, num_heads, k, dropout=dropout)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(start_fm * 2, start_fm, 2, 2)\n",
    "        self.dec1_conv = double_conv(start_fm * 2, start_fm)\n",
    "        self.dec1_vig = ViGBlock(start_fm, start_fm, num_heads, k, dropout=dropout)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(start_fm, 16, kernel_size=1)\n",
    "        self.final_bn = nn.BatchNorm2d(16)\n",
    "        self.final_act = nn.Sigmoid()\n",
    "\n",
    "    def _apply_vig(self, x, vig_module):\n",
    "        B, C, H, W = x.shape\n",
    "        x_flat = x.reshape(B, C, H * W).permute(0, 2, 1)\n",
    "        x_vig = vig_module(x_flat)\n",
    "        return x_vig.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        e1 = self._apply_vig(self.enc1_conv(inputs), self.enc1_vig)\n",
    "        p1 = self.pool1(e1)\n",
    "\n",
    "        e2 = self._apply_vig(self.enc2_conv(p1), self.enc2_vig)\n",
    "        p2 = self.pool2(e2)\n",
    "\n",
    "        e3 = self._apply_vig(self.enc3_conv(p2), self.enc3_vig)\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        e4 = self._apply_vig(self.enc4_conv(p3), self.enc4_vig)\n",
    "        p4 = self.pool4(e4)\n",
    "\n",
    "        b = self._apply_vig(self.bottleneck_conv(p4), self.bottleneck_vig)\n",
    "\n",
    "        d4 = self._apply_vig(self.dec4_conv(torch.cat([self.up4(b), e4], 1)), self.dec4_vig)\n",
    "        d3 = self._apply_vig(self.dec3_conv(torch.cat([self.up3(d4), e3], 1)), self.dec3_vig)\n",
    "        d2 = self._apply_vig(self.dec2_conv(torch.cat([self.up2(d3), e2], 1)), self.dec2_vig)\n",
    "        d1 = self._apply_vig(self.dec1_conv(torch.cat([self.up1(d2), e1], 1)), self.dec1_vig)\n",
    "\n",
    "        return self.final_act(self.final_bn(self.final_conv(d1)))\n",
    "\n",
    "##############################################\n",
    "# 2. Updated PINN Loss (Dice + Physics + Smoothness)\n",
    "##############################################\n",
    "\n",
    "def dice_func(pred, target):\n",
    "    smooth = 1.0\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.reshape(num, -1)\n",
    "    m2 = target.reshape(num, -1)\n",
    "    intersection = (m1 * m2).sum(1)\n",
    "    score = (2. * intersection + smooth) / (m1.pow(2).sum(1) + m2.pow(2).sum(1) + smooth)\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "def smoothness_loss(m):\n",
    "    dx = m[:, :, :, 1:] - m[:, :, :, :-1]\n",
    "    dy = m[:, :, 1:, :] - m[:, :, :-1, :]\n",
    "    return dx.pow(2).mean() + dy.pow(2).mean()\n",
    "\n",
    "\n",
    "def total_pinn_loss(outputs, masks, images, G_matrix,\n",
    "                    lambda_physics=0.01,\n",
    "                    lambda_smooth=0.001):\n",
    "\n",
    "    d_loss = 1 - dice_func(outputs, masks)\n",
    "\n",
    "    batch_size = outputs.size(0)\n",
    "    m_flat = outputs.reshape(batch_size, -1)\n",
    "    d_pred = torch.matmul(m_flat, G_matrix.t())\n",
    "    d_obs = images.reshape(batch_size, -1)\n",
    "\n",
    "    p_loss = F.mse_loss(d_pred, d_obs)\n",
    "    s_loss = smoothness_loss(outputs)\n",
    "\n",
    "    return d_loss + lambda_physics * p_loss + lambda_smooth * s_loss\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 3. Training Setup\n",
    "##############################################\n",
    "\n",
    "lambda_phys = 0.1\n",
    "lambda_smooth = 0.001\n",
    "\n",
    "with h5py.File(name='./G.mat', mode='r') as f:\n",
    "    G = torch.Tensor(np.nan_to_num(f['G'][:])).T.cuda()\n",
    "\n",
    "model = ViGUNet(start_fm=32, num_heads=4, dropout=0.3, k=8).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "mean_tra_losses, mean_val_losses, mean_syn_losses = [], [], []\n",
    "val_data, syn_data = [], []\n",
    "\n",
    "start_time = time.time()\n",
    "epoch = 0\n",
    "\n",
    "while epoch <= epochs:\n",
    "    tra_losses, val_losses, syn_losses = [], [], []\n",
    "\n",
    "    model.train()\n",
    "    for images, masks in tra_loader:\n",
    "        images, masks = images.cuda(), masks.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = total_pinn_loss(\n",
    "            outputs, masks, images, G,\n",
    "            lambda_physics=lambda_phys,\n",
    "            lambda_smooth=lambda_smooth\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tra_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.cuda(), masks.cuda()\n",
    "            outputs = model(images)\n",
    "\n",
    "            v_loss = total_pinn_loss(\n",
    "                outputs, masks, images, G,\n",
    "                lambda_physics=lambda_phys,\n",
    "                lambda_smooth=lambda_smooth\n",
    "            )\n",
    "            val_losses.append(v_loss.item())\n",
    "\n",
    "        for images, masks in syn_loader:\n",
    "            images, masks = images.cuda(), masks.cuda()\n",
    "            outputs = model(images)\n",
    "\n",
    "            s_loss = total_pinn_loss(\n",
    "                outputs, masks, images, G,\n",
    "                lambda_physics=lambda_phys,\n",
    "                lambda_smooth=lambda_smooth\n",
    "            )\n",
    "            syn_losses.append(s_loss.item())\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    m_tra = np.mean(tra_losses)\n",
    "    m_val = np.mean(val_losses)\n",
    "    m_syn = np.mean(syn_losses)\n",
    "\n",
    "    mean_tra_losses.append(m_tra)\n",
    "    mean_val_losses.append(m_val)\n",
    "    mean_syn_losses.append(m_syn)\n",
    "\n",
    "    torch.save(model.state_dict(), 'pinn_vignn_phylossgeoloss.pth')\n",
    "\n",
    "    print(f'Epoch: {epoch:03d} | Tra: {m_tra:.4f} | Val: {m_val:.4f} | Syn: {m_syn:.4f}')\n",
    "\n",
    "print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_File = './pinn_vignn_reg.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "* Loads trained model weights using `load_state_dict()`\n",
    "* Sets model to evaluation mode using `model.eval()`\n",
    "* Disables gradient computation with `torch.no_grad()`\n",
    "* Performs forward pass on **validation dataset**\n",
    "* Performs forward pass on **synthetic dataset**\n",
    "* Stores predicted outputs, ground truth masks, and input images for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for evaluation\n",
    "val_data = []\n",
    "syn_data = []\n",
    "model.load_state_dict(torch.load(model_File),strict=False)\n",
    "model.eval()\n",
    "for images, masks in val_loader:\n",
    "    with torch.no_grad():\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        outputs = model(images)\n",
    "        val_data.extend([[outputs, masks, images]])\n",
    "\n",
    "for images, masks in syn_loader:\n",
    "    with torch.no_grad():\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        outputs = model(images)\n",
    "        syn_data.extend([[outputs, masks, images]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing of Evaluation Results\n",
    "\n",
    "* Extracts predictions, ground truth density, and input gravity data from `val_data`\n",
    "\n",
    "* Reshapes 3D density volumes (16Ã—32Ã—32) into 1D vectors (16384 elements)\n",
    "\n",
    "* Reshapes 2D gravity images (32Ã—32) into 1D vectors (1024 elements)\n",
    "\n",
    "* Converts tensors from GPU to CPU and NumPy format\n",
    "\n",
    "* Stores processed validation results in:\n",
    "\n",
    "  * `val_truth`\n",
    "  * `val_predict`\n",
    "  * `val_truth_d`\n",
    "\n",
    "* Repeats the same processing for synthetic data\n",
    "\n",
    "* Stores processed synthetic results in:\n",
    "\n",
    "  * `syn_truth`\n",
    "  * `syn_predict`\n",
    "  * `syn_truth_d`\n",
    "\n",
    "This prepares the outputs for metric computation and quantitative evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data = np.array(val_data)\n",
    "val_truth = []\n",
    "val_predict = []\n",
    "val_truth_d = []\n",
    "for p, q in enumerate(val_data):\n",
    "    for i, j in enumerate(q[1]):\n",
    "        val_truth.append(j.reshape(1, 16384)[0].cpu().numpy())\n",
    "    for i, j in enumerate(q[0]):\n",
    "        val_predict.append(j.reshape(1, 16384)[0].cpu().numpy())\n",
    "    for i, j in enumerate(q[2]):\n",
    "        val_truth_d.append(j.reshape(1, 1024)[0].cpu().numpy())\n",
    "\n",
    "# syn_data = np.array(syn_data)\n",
    "syn_truth = []\n",
    "syn_predict = []\n",
    "syn_truth_d = []\n",
    "for p, q in enumerate(syn_data):\n",
    "    for i, j in enumerate(q[1]):\n",
    "        syn_truth.append(j.reshape(1, 16384)[0].cpu().numpy())\n",
    "    for i, j in enumerate(q[0]):\n",
    "        syn_predict.append(j.reshape(1, 16384)[0].cpu().numpy())\n",
    "    for i, j in enumerate(q[2]):\n",
    "        syn_truth_d.append(j.reshape(1, 1024)[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16384,)\n",
      "(16384,)\n",
      "(1024,)\n",
      "(16384,)\n",
      "(16384,)\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "print(val_truth[0].shape)\n",
    "print(val_predict[0].shape)\n",
    "print(val_truth_d[0].shape)\n",
    "\n",
    "print(syn_truth[0].shape)\n",
    "print(syn_predict[0].shape)\n",
    "print(syn_truth_d[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation & Metrics Computation\n",
    "\n",
    "* Converts forward matrix **G** to NumPy format for metric calculation\n",
    "* Defines `compute_metrics()` to evaluate model performance\n",
    "\n",
    "#### ðŸ”¹ For Each Sample:\n",
    "\n",
    "* Flattens predicted and true density models\n",
    "* Computes **Relative Error (L2 norm)** between prediction and ground truth\n",
    "* Reconstructs gravity data using forward modeling:\n",
    "  [\n",
    "  d_{pred} = G \\cdot (\\rho \\cdot m_{pred})\n",
    "  ]\n",
    "* Computes **RÂ² score** to measure physical data fidelity\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Validation Evaluation\n",
    "\n",
    "* Computes mean Relative Error and RÂ² for validation dataset\n",
    "* Stores results in `E` and `R_2`\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Synthetic Evaluation\n",
    "\n",
    "* Computes metrics for full synthetic dataset\n",
    "* Segments results into predefined geological categories\n",
    "* Calculates mean metrics for each category\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Final Output\n",
    "\n",
    "* Displays tabulated results including:\n",
    "\n",
    "  * Validation performance\n",
    "  * Category-wise synthetic performance\n",
    "* Reports:\n",
    "\n",
    "  * Relative Error (E)\n",
    "  * RÂ² Score (physical consistency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Initialize lists to store mean results\n",
    "E = []\n",
    "R_2 = []\n",
    "\n",
    "# Ensure G and density are accessible and in numpy format\n",
    "G_arr = G.cpu().numpy() if torch.is_tensor(G) else np.array(G)\n",
    "\n",
    "def compute_metrics(predict_list, truth_list, truth_d_list, G_mat, den):\n",
    "    rel_list = []\n",
    "    r2_list = []\n",
    "    \n",
    "    for i in range(len(predict_list)):\n",
    "        # Convert to numpy and flatten\n",
    "        # We use .detach().cpu() in case they are still torch tensors\n",
    "        p = predict_list[i]\n",
    "        t = truth_list[i]\n",
    "        d_obs = truth_d_list[i]\n",
    "        \n",
    "        m_pre = p.detach().cpu().numpy().flatten() if torch.is_tensor(p) else np.array(p).flatten()\n",
    "        m_tru = t.detach().cpu().numpy().flatten() if torch.is_tensor(t) else np.array(t).flatten()\n",
    "        tru_d = d_obs.detach().cpu().numpy().flatten() if torch.is_tensor(d_obs) else np.array(d_obs).flatten()\n",
    "\n",
    "        # 1. Relative Error (L2 Norm)\n",
    "        rel = np.linalg.norm(m_pre - m_tru) / (np.linalg.norm(m_tru) + 1e-8)\n",
    "        rel_list.append(rel)\n",
    "\n",
    "        # 2. R^2 Score (Physical Fidelity)\n",
    "        # Reconstruct data: d_pred = G * (density * m_pred)\n",
    "        pre_d = G_mat @ (den * m_pre)\n",
    "        \n",
    "        res = tru_d - pre_d\n",
    "        ss_res = np.sum(res**2)\n",
    "        ss_tot = np.sum((tru_d - np.mean(tru_d))**2)\n",
    "        \n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        r2_list.append(r2)\n",
    "        \n",
    "    return rel_list, r2_list\n",
    "\n",
    "# --- 1. Evaluate Validation Set ---\n",
    "val_rel, val_R_2 = compute_metrics(val_predict, val_truth, val_truth_d, G_arr, density)\n",
    "E.append(np.mean(val_rel))\n",
    "R_2.append(np.mean(val_R_2))\n",
    "\n",
    "# --- 2. Evaluate Synthesis Set (6 Categories) ---\n",
    "syn_rel, syn_R_2 = compute_metrics(syn_predict, syn_truth, syn_truth_d, G_arr, density)\n",
    "\n",
    "# --- 3. Segment Results by Category ---\n",
    "for i in range(category):\n",
    "    start = i * part_num\n",
    "    end = (i + 1) * part_num\n",
    "    \n",
    "    E.append(np.mean(syn_rel[start:end]))\n",
    "    R_2.append(np.mean(syn_R_2[start:end]))\n",
    "\n",
    "# --- 4. Display Results ---\n",
    "print(f\"{'Data Group':<15} | {'Rel. Error (E)':<15} | {'R2 Score':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Validation':<15} | {E[0]:<15.4f} | {R_2[0]:<10.4f}\")\n",
    "for i in range(1, category + 1):\n",
    "    print(f\"{'Category ' + str(i):<15} | {E[i]:<15.4f} | {R_2[i]:<10.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimesh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
